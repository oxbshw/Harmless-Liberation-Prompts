# Harmless Liberation Prompts: A Research Repository for AI Safety and Adversarial Testing

[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)

**This repository is for educational and research purposes only.**

---

## Table of Contents

- [Project Description](#project-description)
- [Ethical Considerations and Responsible Disclosure](#ethical-considerations-and-responsible-disclosure)
- [Getting Started](#getting-started)
- [Usage Guidelines](#usage-guidelines)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## Project Description

This repository contains a collection of prompts designed for studying and evaluating the safety, robustness, and content filtering mechanisms of various Large Language Models (LLMs). These prompts, commonly referred to as "jailbreaks" or adversarial inputs, are intended for use in controlled research environments.

The primary goals of this project are:
- To provide a centralized resource for researchers to better understand LLM vulnerabilities.
- To facilitate the testing and analysis of AI safety filters and alignment techniques.
- To encourage the development of more secure, robust, and resilient AI systems.

This project is aimed at AI researchers, security professionals, and developers working to improve the safety and reliability of AI models. It is not intended to be used for generating harmful or malicious content.

---

## Ethical Considerations and Responsible Disclosure

The contents of this repository come with significant responsibility. All users and contributors are expected to adhere to the following ethical guidelines.

### üìú **Disclaimer**
The prompts and techniques contained in this repository are provided "as-is" for academic and research purposes. The maintainers of this repository are not responsible for the actions of users or for any content generated by AI models in response to these prompts. By using this repository, you agree to take full responsibility for your own actions and their consequences.

### ‚ö†Ô∏è **Warning Against Misuse**
The use of these prompts to generate content that is hateful, illegal, unethical, or malicious is strictly prohibited. This repository is a tool for research and defense, not for attack. Misuse of this content is a violation of the spirit and intent of this project.

### üõ°Ô∏è **Responsible Disclosure**
If you discover a significant or novel vulnerability in a commercial or open-source AI model using the techniques presented here, we **strongly encourage** you to report it directly to the respective AI vendor or developer. Most major AI labs have official bug bounty or vulnerability disclosure programs designed for this purpose. Responsible disclosure helps ensure that vulnerabilities are addressed and contributes to a safer AI ecosystem for everyone.

---

## Getting Started

The repository is a collection of Markdown (`.mkd`) files containing prompts. No special installation is required. To get a local copy, simply clone the repository:

```bash
git clone https://github.com/oxbshw/Harmless-Liberation-Prompts.git
```

---

## Usage Guidelines

The prompts are organized into `.mkd` files, typically named after the AI company or model they are designed for.

### How to Use
1.  Navigate to the file corresponding to the model you are researching.
2.  Select a prompt to analyze or use in your testing environment.
3.  Copy the prompt and use it as input for the target LLM in a controlled and isolated setting.
4.  Analyze the model's response to evaluate its adherence to safety protocols.

### Example of Analysis (Conceptual)
Researchers can use these prompts to understand common adversarial techniques. For example, a prompt might employ a combination of:
- **Persona Injection:** Instructing the model to adopt a specific character to bypass its default conversational constraints.
- **Hypothetical Scenarios:** Framing a sensitive request within a "hypothetical" or "fictional" context.
- **Obfuscation:** Using techniques like leetspeak or special characters to hide keywords from content filters.

It is crucial to use these prompts exclusively in environments where the generated output will not cause harm.

---

## Contributing

We welcome contributions from the AI research and security communities. If you would like to contribute, please follow these steps:

1.  **Fork the repository.**
2.  **Create a new branch** for your changes (`git checkout -b feature/YourContribution`).
3.  **Add your prompts or improvements.** Ensure that any new prompts have a clear research application and are documented appropriately.
4.  **Open a Pull Request.** Please provide a clear description of your contribution and its relevance to AI safety research.

Before contributing, please open an issue to discuss your proposed changes with the maintainers. All contributions must align with the project's ethical guidelines.

---

## License

This project is licensed under the **AGPL-3.0 License**. Please see the [LICENSE](LICENSE) file for more details. This license was chosen to ensure that any derivative works remain open and accessible to the community.

---

## Contact

For questions, suggestions, or to discuss the project, please open a GitHub issue in this repository.
